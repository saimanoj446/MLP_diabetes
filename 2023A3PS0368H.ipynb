{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0224f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:04:15.357067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763973255.429500   80074 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763973255.448940   80074 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1763973255.613077   80074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763973255.613106   80074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763973255.613108   80074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763973255.613111   80074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-24 14:04:15.638674: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a852f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Glucose  Insulin   BMI  Age  Outcome\n",
      "0      148        0  33.6   50        1\n",
      "1       85        0  26.6   31        0\n",
      "2      183        0  23.3   32        1\n",
      "3       89       94  28.1   21        0\n",
      "4      137      168  43.1   33        1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5affb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>120.894531</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>31.972618</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>117.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140.250000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>199.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Glucose     Insulin         BMI         Age     Outcome\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.000000\n",
       "mean   120.894531   79.799479   31.992578   33.240885    0.348958\n",
       "std     31.972618  115.244002    7.884160   11.760232    0.476951\n",
       "min      0.000000    0.000000    0.000000   21.000000    0.000000\n",
       "25%     99.000000    0.000000   27.300000   24.000000    0.000000\n",
       "50%    117.000000   30.500000   32.000000   29.000000    0.000000\n",
       "75%    140.250000  127.250000   36.600000   41.000000    1.000000\n",
       "max    199.000000  846.000000   67.100000   81.000000    1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3060ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels='Outcome',axis=1)\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1191a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6b342",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6ee528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "\n",
    "model.fit(X_train_scaled,y_train)\n",
    "\n",
    "y_pred_GNB = model.predict(X_test_scaled)\n",
    "print(y_pred_GNB[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897eddec",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self,learning_rate = 0.01, n_iterations = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    def fit(self, X, y):\n",
    "        n_samples ,n_features = X.shape\n",
    "        \n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "               \n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                \n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias \n",
    "\n",
    "                y_predicted = np.sign(linear_output)\n",
    "\n",
    "                if y_predicted != y_[idx]:\n",
    "                    update = self.learning_rate * y_[idx]\n",
    "                    self.weights += update * x_i\n",
    "                    self.bias += update\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = np.sign(linear_output)\n",
    "        \n",
    "        return np.where(y_predicted == -1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b6300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "custom_mlp = Perceptron(learning_rate=0.01,n_iterations=1000)\n",
    "custom_mlp.fit(X_train_scaled,y_train)\n",
    "\n",
    "y_pred_mlp = custom_mlp.predict(X_test_scaled)\n",
    "print(y_pred_mlp[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdffe314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from GNB:\n",
      "Accuracy_GNB: 0.7467532467532467\n",
      "Precision_GNB: 0.6481481481481481\n",
      "Recall_GNB: 0.6363636363636364\n",
      "F1_GNB: 0.6422018348623854\n",
      "\n",
      "\n",
      "Metrics from MLP:\n",
      "Accuracy_MLP: 0.6753246753246753\n",
      "Precision_MLP: 0.5301204819277109\n",
      "Recall_MLP: 0.8\n",
      "F1_MLP: 0.6376811594202898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "accuracy_GNB = accuracy_score(y_test,y_pred_GNB)\n",
    "precision_GNB = precision_score(y_test,y_pred_GNB)\n",
    "recall_GNB = recall_score(y_test,y_pred_GNB)\n",
    "f1_GNB = f1_score(y_test,y_pred_GNB)\n",
    "\n",
    "print(\"Metrics from GNB:\")\n",
    "print(f'Accuracy_GNB: {accuracy_GNB}')\n",
    "print(f'Precision_GNB: {precision_GNB}')\n",
    "print(f'Recall_GNB: {recall_GNB}')\n",
    "print(f'F1_GNB: {f1_GNB}')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Metrics from MLP:\")\n",
    "\n",
    "accuracy_MLP = accuracy_score(y_test,y_pred_mlp)\n",
    "precision_MLP = precision_score(y_test,y_pred_mlp)\n",
    "recall_MLP = recall_score(y_test,y_pred_mlp)\n",
    "f1_MLP = f1_score(y_test,y_pred_mlp)\n",
    "\n",
    "print(f'Accuracy_MLP: {accuracy_MLP}')\n",
    "print(f'Precision_MLP: {precision_MLP}')\n",
    "print(f'Recall_MLP: {recall_MLP}')\n",
    "print(f'F1_MLP: {f1_MLP}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8f356",
   "metadata": {},
   "source": [
    "1. Accuracy: Gaussian NaÃ¯ve Bayes (74.7%) is significantly more accurate than the MLP (67.5%).\n",
    "\n",
    "Reason: The diabetes dataset is not \"linearly separable\". The MLP is trying to force a straight line through complex data, whereas GNB uses probability curves (Gaussian distribution), allowing it to capture the \"shape\" of the data better.\n",
    "\n",
    "2. Precision vs. Recall Trade-off\n",
    "This is the most interesting part of your results.\n",
    "\n",
    "Precision (GNB is cleaner): GNB (64.8%) vs MLP (53.0%).\n",
    "\n",
    "Reason: When the MLP predicts someone has diabetes, it is wrong nearly half the time (low precision). It generates a lot of \"False Positives.\" GNB is much more trustworthy when it claims a patient is sick.\n",
    "\n",
    "Recall (MLP is better): MLP (80.0%) vs GNB (63.6%).\n",
    "\n",
    "Interpretation: MLP is actually better at catching sick people. It identified 80% of the diabetic patients in the test set, whereas GNB missed nearly 40% of them.\n",
    "\n",
    "Which is more significant, Recall or Precision: In medical screening, Recall is often prioritized over Precision. You would rather falsely warn a healthy person (False Positive) than tell a sick person they are fine (False Negative).\n",
    "\n",
    "3. F1-Score: The scores are almost identical (~64%).\n",
    "\n",
    "Interpretation: This shows that while the models behave very differently, their \"overall\" effectiveness is similar. GNB is balanced; MLP sacrifices precision to get high recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8de6ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be0c5db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "with open('naive_bayes_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "with open('mlp_model.pkl', 'wb') as file:\n",
    "    pickle.dump(custom_mlp, file)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "print(\"Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b871fb8-dd51-4f06-a344-15d8de26fffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes 5-Fold CV:\n",
      "--- Fold 1/5 ---\n",
      "Accuracy for this fold: 77.24%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Accuracy for this fold: 75.61%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Accuracy for this fold: 73.17%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Accuracy for this fold: 72.36%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Accuracy for this fold: 74.59%\n",
      "\n",
      "--- K-Fold Cross-Validation Results ---\n",
      "Average Accuracy: 74.59%\n",
      "Standard Deviation: 1.73%\n",
      "\n",
      "========================================\n",
      "\n",
      "Custom MLP 5-Fold CV:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 1/5 ---\n",
      "Accuracy for this fold: 57.72%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Accuracy for this fold: 73.98%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Accuracy for this fold: 71.54%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Accuracy for this fold: 71.54%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Accuracy for this fold: 76.23%\n",
      "\n",
      "--- K-Fold Cross-Validation Results ---\n",
      "Average Accuracy: 70.21%\n",
      "Standard Deviation: 6.48%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Gaussian Naive Bayes 5-Fold CV:\")\n",
    "\n",
    "gnb_scores = cross_val_score(model, X_train_scaled, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "for i, score in enumerate(gnb_scores, 1):\n",
    "    print(f\"--- Fold {i}/5 ---\")\n",
    "    print(f\"Accuracy for this fold: {score*100:.2f}%\\n\")\n",
    "\n",
    "print(\"--- K-Fold Cross-Validation Results ---\")\n",
    "print(f\"Average Accuracy: {gnb_scores.mean()*100:.2f}%\")\n",
    "print(f\"Standard Deviation: {gnb_scores.std()*100:.2f}%\")\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "print(\"Custom MLP 5-Fold CV:\")\n",
    "\n",
    "mlp_accuracies = []\n",
    "y_train_np = y_train.to_numpy()\n",
    "\n",
    "fold_num = 1\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_fold_train, y_fold_val = y_train_np[train_index], y_train_np[val_index]\n",
    "    \n",
    "    fold_model = Perceptron(learning_rate=0.01, n_iterations=1000)\n",
    "    fold_model.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    predictions = fold_model.predict(X_fold_val)\n",
    "    accuracy = np.mean(predictions == y_fold_val)\n",
    "    mlp_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"--- Fold {fold_num}/5 ---\")\n",
    "    print(f\"Accuracy for this fold: {accuracy*100:.2f}%\\n\")\n",
    "    fold_num += 1\n",
    "\n",
    "mlp_accuracies = np.array(mlp_accuracies)\n",
    "\n",
    "print(\"--- K-Fold Cross-Validation Results ---\")\n",
    "print(f\"Average Accuracy: {mlp_accuracies.mean()*100:.2f}%\")\n",
    "print(f\"Standard Deviation: {mlp_accuracies.std()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "456caa4f-655f-49eb-9c6b-d9d5ad209edb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "1. Stability and Robustness (K-Fold Results)\n",
    "\n",
    "    Gaussian Naive Bayes displayed high stability across all 5 folds with a low standard deviation. This indicates the model generalizes well and is not overfitted to specific training data.\n",
    "\n",
    "    MLP showed slightly higher variance between folds. As a linear classifier trying to fit non-linear data, its performance fluctuates more depending on which data points are in the training set.\n",
    "\n",
    "2. Accuracy vs. Reliability (The Trade-off)\n",
    "\n",
    "    Winner for Accuracy: Gaussian Naive Bayes (~74-76%)\n",
    "\n",
    "        GNB consistently achieves higher overall accuracy than the MLP (~67%).\n",
    "\n",
    "        Reasoning: The diabetes dataset features are not linearly separable. GNB uses probability distributions (Gaussian) to capture the \"shape\" of the data classes, whereas the simple Perceptron tries to draw a straight line, which leads to more errors.\n",
    "\n",
    "3. The \"Medical Safety\" Perspective (Precision vs. Recall)\n",
    "\n",
    "While GNB is more accurate overall, the MLP has a critical advantage relevant to healthcare: Recall.\n",
    "\n",
    "    MLP Recall: ~80%, GNB Recall: ~63%\n",
    "\n",
    "    Implication: In a medical context, Recall is often more important than Precision. We would rather falsely flag a healthy person (False Positive) than miss a sick person (False Negative).\n",
    "\n",
    "        The MLP acts as a better \"Safety Net\", catching 80% of diabetic cases.\n",
    "\n",
    "        The GNB model, while more precise, missed nearly 40% of the positive cases in the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
